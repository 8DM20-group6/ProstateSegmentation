#%%
import random
from pathlib import Path

import torch
from torch.utils.data import DataLoader
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

import u_net
import utils

# to ensure reproducible training/validation split
random.seed(42)

# find out if a GPU is available
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")

# directorys with data and to store training checkpoints and logs
DATA_DIR = Path.cwd().parent / "TrainingData"
CHECKPOINTS_DIR = Path.cwd() / "segmentation_model_weights"
CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)
TENSORBOARD_LOGDIR = "segmentation_runs"

# training settings and hyperparameters
NO_VALIDATION_PATIENTS = 2
IMAGE_SIZE = [64, 64]
BATCH_SIZE = 32
N_EPOCHS = 20
LEARNING_RATE = 1e-4
TOLERANCE = 0.03  # for early stopping

# find patient folders in training directory
# excluding hidden folders (start with .)
patients = [
    path
    for path in DATA_DIR.glob("*")
    if not any(part.startswith(".") for part in path.parts)
]
random.shuffle(patients)

# split in training/validation after shuffling
partition = {
    "train": patients[:-NO_VALIDATION_PATIENTS],
    "validation": patients[-NO_VALIDATION_PATIENTS:],
}

# load training data and create DataLoader with batching and shuffling
dataset = utils.ProstateMRDataset(partition["train"], IMAGE_SIZE)
dataloader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    pin_memory=False,
)

# load validation data
valid_dataset = utils.ProstateMRDataset(partition["validation"], IMAGE_SIZE)
valid_dataloader = DataLoader(
    valid_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    pin_memory=False,
)
#%%
# initialise model, optimiser, and loss function
# loss_function = nn.CrossEntropyLoss() # TODO: import custom loss function from utils module 
loss_function = utils.DiceBCELoss()
unet_model = u_net.UNet().to(device) # TODO: import unet implementation from unet module
optimizer = torch.optim.Adam(unet_model.parameters(), lr=0.0001) # TODO: use a default pytorch optimiser

minimum_valid_loss = 10  # initial validation loss
writer = SummaryWriter(log_dir=TENSORBOARD_LOGDIR)  # tensorboard summary
#%%
# training loop
for epoch in range(N_EPOCHS):
    current_train_loss = 0.0
    current_valid_loss = 0.0
    
    # TODO: training iterations
    # TODO: research the required implementation of training iterations in pytorch
    # usually consisting of (1) zeroing the gradients, (2) forward pass of model,
    # (3) computing loss, (4) backpropagating, (5) stepping the optimiser
    unet_model.train()    
    with tqdm(dataloader, unit="batch") as tepoch:
        tepoch.set_description(f"Epoch: {epoch+1}/{N_EPOCHS}")        
        for img, mask in tepoch:
            optimizer.zero_grad() # 1   
            img, mask = img.to(device), mask.float().to(device)
            prediction = unet_model(img) # 2
            loss = loss_function(prediction, mask) # 3
            current_train_loss+=loss
            loss.backward()
            optimizer.step()

    # evaluate validation loss
    with torch.no_grad():
        unet_model.eval() # turns off the training setting to allow evaluation 
        for img, mask in valid_dataloader:
            img, mask = img.to(device), mask.float().to(device)
            prediction = unet_model(img)
            loss = loss_function(prediction, mask) # 3
            current_valid_loss+=loss
        unet_model.train() # turns training setting back on

    # write to tensorboard log
    writer.add_scalar("Loss/train", current_train_loss / len(dataloader), epoch)
    writer.add_scalar(
        "Loss/validation", current_valid_loss / len(valid_dataloader), epoch
    )

    # if validation loss is improving, save model checkpoint
    # only start saving after 10 epochs
    if (current_valid_loss / len(valid_dataloader)) < minimum_valid_loss + TOLERANCE:
        minimum_valid_loss = current_valid_loss / len(valid_dataloader)
        weights_dict = {k: v.cpu() for k, v in unet_model.state_dict().items()}
        if epoch > 9:
            torch.save(
                weights_dict,
                CHECKPOINTS_DIR / f"u_net_{epoch}.pth",
            )
